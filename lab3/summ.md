**Отчёт по дообучению RuBERT на датасете Sib-200**

**Классы**: entertainment, geography, health, politics, science/technology, sports, travel

Исходный датасет сильно несбалансирован:
    Минимум: geography — 58 примеров  
    Максимум: science/technology — 176 примеров
    Это приводит к смещению модели в сторону частых классов

1. **Аугментация данных с помощью Qwen**

    Локально юзал Qwen1.5-1.8B-Chat
    Для каждого класса догенерено 120 примеров
    Нагенерил промптов для генерации новых экземпляров классов
    Сохранил в synthetic_sib200.json

2. **Создание сбалансированного обучающего набора**

    Объединены оригинальные и синтетические данные
    Итоговое распределение: 120 и больше примеров на каждый класс

3. **Дообучение RuBERT**

    Базовая модель: DeepPavlov/rubert-base-cased.
    Параметры обучения:
        batch_size = 32
        max_length = 128
        num_train_epochs = 8
        learning_rate = 2e-5
        warmup_ratio = 0.1, lr_scheduler = cosine
  ??? Weights & Biases ???


Результаты
    Датасет сбалансирован
    Модель обучена с учётом синтетических данных
    Проведена полная оценка на отложенных выборках

Пробовал, но не помогло:
    Заменить metric_for_best_model='eval_loss' -> 'eval_f1'
    Реализовать взвешенную функцию потерь (class_weight='balanced')
    
Хотел попробовать:
    Провести анализ ошибок по классам
