**Че было сделано:**
1. Смена модели: с xlm-roberta-base на adsabs/astroBERT
Было:
    Использовалась общая мультиязычная модель FacebookAI/xlm-roberta-base.
    Это приводило к низкой точности распознавания специфических астрофизических сущностей (например, названий телескопов, каталогов, инструментов)
Стало:
    Замена на специализированную модель adsabs/astroBERT, предобученную на корпусе научных статей из Astrophysics Data System 
    Модель изначально «знает» термины 
    Это привело к значительному росту качества NER на целевом домене
    
    Почему сработало: повышение precision и recall за счёт доменной адаптации модели.

2. Уточнение настроек обучения под астрофизический домен
Было:
    Обучение xlm-roberta-base с гиперпараметрами, не оптимизированными под малый, узкоспециализированный датасет
    Высокий learning_rate=1e-4 и warmup_ratio=0.5 — избыточны для fine-tuning уже предобученной на домене модели

Стало:
    Для astroBERT использованы более осторожные настройки:
        learning_rate=2e-5 (стандарт для fine-tuning BERT-моделей)
        warmup_ratio=0.1 
        per_device_train_batch_size=16 (оптимально для средних объемов RAM(32 на моей машинке справились))
    Обучение стало быстрее и стабильнее

    Почему: избежали переобучения и ускорили сходимость
    
3. Корректная оценка на тестовом сете
Было:
    При использовании xlm-roberta-base оценка проводилась через ручной цикл с pipeline, что:
        не учитывало доменную специфику токенизации,
        приводило к ошибкам при длине >512 токенов,
        давало некорректные метрики.

Стало:
    Внедрён полный пайплайн оценки через Trainer.predict() с:
        корректной токенизацией (is_split_into_words=True, truncation=True)
        выравниванием через word_ids
        использованием id2label из конфига модели

    ПОЧЕМУ: метрики теперь точно отражают реальное качество модели на целевом датасете.

4. Отказ от мультиязычности в пользу доменной точности
Было:
    xlm-roberta-base - мультиязычная, но слабая в узких доменах
    Мультиязычность уменьшала ёмкость модели для англоязычных научных терминов

Стало:
    Использование моноязычной, но доменно-специфичной модели astroBERT
    Фокус на качество, а не на широту охвата

    Почему лучше: модель стала лучше понимать контекст научных публикаций.

5. Улучшения общие:
    Все параметры (модель, learning rate, batch size) явно заданы
    Оценка проводится строго на том же формате данных, что и обучение
    Используется оригинальный id2label из чекпоинта
